import os
import asyncio
# ==========================================
# âš¡ï¸ ç½‘ç»œç¯å¢ƒé…ç½®
# ==========================================
# 1. å…è®¸è®¿é—® HuggingFace é•œåƒ (ä¸‹è½½æ¨¡å‹ç”¨)
os.environ['HF_ENDPOINT'] = 'https://hf-mirror.com'

# 2. å¼ºåˆ¶æœ¬åœ°æµé‡ä¸èµ°ä»£ç† (è§£å†³ 502/404 æŠ¥é”™çš„å…³é”®)
os.environ['NO_PROXY'] = 'localhost,127.0.0.1,0.0.0.0'

from fastapi import FastAPI
from fastapi.responses import StreamingResponse
from fastapi.middleware.cors import CORSMiddleware

# LangChain ç›¸å…³åº“
from langchain_community.vectorstores import FAISS
from langchain_community.embeddings import HuggingFaceBgeEmbeddings
from langchain_ollama import ChatOllama
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser

app = FastAPI()

# è·¨åŸŸé…ç½®
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# =======================================================
# å…¨å±€èµ„æºåŠ è½½
# =======================================================
VECTOR_DB_PATH = "./vector_db"
embeddings = None
vector_store = None
llm = None

@app.on_event("startup")
async def startup_event():
    global embeddings, vector_store, llm
    print("æ­£åœ¨åˆå§‹åŒ–ç³»ç»Ÿèµ„æº...")
    
    # A. åŠ è½½ Embedding
    EMBEDDING_MODEL_NAME = "BAAI/bge-small-zh-v1.5" 
    print(f"1. åŠ è½½ Embedding æ¨¡å‹: {EMBEDDING_MODEL_NAME}...")
    try:
        embeddings = HuggingFaceBgeEmbeddings(
            model_name=EMBEDDING_MODEL_NAME,
            model_kwargs={'device': 'cpu'},
            encode_kwargs={'normalize_embeddings': True}
        )
    except Exception as e:
        print(f"âŒ Embedding åŠ è½½å¤±è´¥: {e}")

    # B. åŠ è½½å‘é‡åº“
    print(f"2. åŠ è½½å‘é‡æ•°æ®åº“: {VECTOR_DB_PATH}...")
    if os.path.exists(VECTOR_DB_PATH):
        try:
            vector_store = FAISS.load_local(
                VECTOR_DB_PATH, 
                embeddings, 
                allow_dangerous_deserialization=True
            )
            print("âœ… å‘é‡åº“åŠ è½½æˆåŠŸï¼")
        except Exception as e:
            print(f"âŒ å‘é‡åº“åŠ è½½å‡ºé”™: {e}")
    else:
        print("âŒ æœªæ‰¾åˆ° vector_db æ–‡ä»¶å¤¹ï¼Œè¯·å…ˆè¿è¡Œ ingest.py")

    # C. è¿æ¥ Ollama
    print("3. è¿æ¥æœ¬åœ° Ollama æœåŠ¡...")
    llm = ChatOllama(
        model="qwen2.5:7b",  # ç¡®ä¿å’Œä½  ollama list é‡Œåå­—ä¸€è‡´
        temperature=0.3,     
    )
    print("âœ… ç³»ç»Ÿåˆå§‹åŒ–å®Œæˆï¼Œç­‰å¾…è¯·æ±‚...")


# =======================================================
# RAG æ ¸å¿ƒç”Ÿæˆé€»è¾‘ (è¿™é‡Œä¿®å¤äº† chain ä¸¢å¤±çš„é—®é¢˜)
# =======================================================
async def rag_stream_generator(query: str):
    # 1. æ£€ç´¢é˜¶æ®µ
    yield "ã€åç«¯ã€‘æ­£åœ¨æ£€ç´¢çŸ¥è¯†åº“...\n"
    await asyncio.sleep(0.1)
    
    if not vector_store:
        yield "âŒ é”™è¯¯ï¼šå‘é‡åº“æœªåŠ è½½ã€‚\n"
        return

    # ä½¿ç”¨ MMR ç®—æ³•æ£€ç´¢ (ä¼˜åŒ–åŒ¹é…åº¦)
    try:
        docs = vector_store.search(
            query, 
            search_type="mmr", 
            k=4, 
            search_kwargs={"fetch_k": 10}
        )
    except Exception as e:
        yield f"âŒ æ£€ç´¢å‡ºé”™: {str(e)}\n"
        return
    
    if not docs:
        yield "âš ï¸ æœªæ‰¾åˆ°ç›¸å…³èµ„æ–™ï¼Œå°è¯•é€šç”¨å›ç­”...\n\n"
        context = "æ— ç›¸å…³èƒŒæ™¯çŸ¥è¯†ã€‚"
    else:
        yield f"âœ… å·²æ‰¾åˆ° {len(docs)} æ¡ç›¸å…³èµ„æ–™ï¼Œæ­£åœ¨é˜…è¯»...\n\n"
        context = "\n\n".join([doc.page_content for doc in docs])

    # 2. ç”Ÿæˆé˜¶æ®µ (æ–°ç‰ˆæç¤ºè¯)
    template = """ä½ æ˜¯ä¸€åä¸“ä¸šçš„æ ¡å›­æ•™åŠ¡åŠ©æ‰‹ã€‚ä½ çš„ä»»åŠ¡æ˜¯åŸºäºä¸‹æ–¹çš„ã€å‚è€ƒèµ„æ–™ã€‘å›ç­”åŒå­¦çš„é—®é¢˜ã€‚

è¯·éµå®ˆä»¥ä¸‹è§„åˆ™ï¼š
1. **ä¾æ®äº‹å®**ï¼šåªèƒ½æ ¹æ®ã€å‚è€ƒèµ„æ–™ã€‘çš„å†…å®¹å›ç­”ï¼Œä¸¥ç¦ä½¿ç”¨ä½ è‡ªå¸¦çš„é€šç”¨çŸ¥è¯†çç¼–ã€‚
2. **æ³¨æ˜æ¥æº**ï¼šå›ç­”æ—¶è¯·å°½é‡è‡ªç„¶åœ°æåŠèµ„æ–™æ¥æºï¼ˆä¾‹å¦‚ï¼šâ€œæ ¹æ®æ•™åŠ¡å¤„æ–‡ä»¶è§„å®š...â€ï¼‰ã€‚
3. **è¯šå®åŸåˆ™**ï¼šå¦‚æœã€å‚è€ƒèµ„æ–™ã€‘ä¸­æ²¡æœ‰åŒ…å«é—®é¢˜çš„ç­”æ¡ˆï¼Œè¯·ç›´æ¥å›å¤ï¼šâ€œæŠ±æ­‰ï¼Œå½“å‰çš„çŸ¥è¯†åº“ä¸­æœªæ‰¾åˆ°ç›¸å…³ä¿¡æ¯ã€‚â€
4. **è¯­æ°”é£æ ¼**ï¼šäº²åˆ‡ã€å®¢è§‚ã€æœ‰æ¡ç†ã€‚

ã€å‚è€ƒèµ„æ–™ã€‘ï¼š
{context}

ã€åŒå­¦çš„é—®é¢˜ã€‘ï¼š
{question}

è¯·å¼€å§‹ä½œç­”ï¼š"""

    # -------------------------------------------------------
    # ğŸ‘‡ å°±æ˜¯è¿™é‡Œï¼ä¹‹å‰æŠ¥é”™æ˜¯å› ä¸ºä¸‹é¢è¿™ä¸¤è¡Œè¢«è¯¯åˆ äº†
    # -------------------------------------------------------
    prompt = ChatPromptTemplate.from_template(template)
    chain = prompt | llm | StrOutputParser()
    # -------------------------------------------------------

    # 3. æµå¼è°ƒç”¨
    try:
        async for chunk in chain.astream({"context": context, "question": query}):
            yield chunk
    except Exception as e:
        yield f"\n\nâŒ Ollama è°ƒç”¨å¤±è´¥: {str(e)}\nè¯·æ£€æŸ¥ Ollama æ˜¯å¦åœ¨è¿è¡Œã€‚"

@app.get("/chat")
async def chat(query: str):
    print(f"æ”¶åˆ°è¯·æ±‚: {query}")
    return StreamingResponse(rag_stream_generator(query), media_type="text/plain")

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)